{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aefcdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 16:21:30.639460: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-04 16:21:30.663369: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b406542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth enabled for RTX 5060 Ti 16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 16:22:04.042090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-04 16:22:04.049243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-04 16:22:04.052007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-09-04 16:22:04.053913: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2049] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "def setup_gpu():\n",
    "    \"\"\"Configure RTX 5060 Ti for optimal performance\"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled for RTX 5060 Ti 16GB\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU setup error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76563bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory between trainings\"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5bb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q64_GridSearch:\n",
    "    def __init__(self, data_path):\n",
    "        setup_gpu()\n",
    "        \n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.target_feature = 'q64'\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.results = []\n",
    "        \n",
    "        q64_data = self.df[self.target_feature].values.reshape(-1, 1)\n",
    "        self.scaled_data = self.scaler.fit_transform(q64_data).flatten()\n",
    "        \n",
    "        print(f\"Data loaded: {len(self.df):,} samples\")\n",
    "        print(f\"Using UNIVARIATE time series: {self.target_feature} only\")\n",
    "        print(f\"Data range: {self.df[self.target_feature].min():.2f} to {self.df[self.target_feature].max():.2f}\")\n",
    "        \n",
    "    def create_univariate_dataset(self, past_window, future_window):\n",
    "        \"\"\"Create univariate time series dataset\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(len(self.scaled_data) - past_window - future_window):\n",
    "            # Input: past_window values of q64\n",
    "            X_window = self.scaled_data[i:i+past_window]\n",
    "            # Output: future_window values of q64\n",
    "            y_sequence = self.scaled_data[i+past_window:i+past_window+future_window]\n",
    "            \n",
    "            X.append(X_window)\n",
    "            y.append(y_sequence)\n",
    "        \n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "        \n",
    "        # Reshape X for RNN: (samples, timesteps, features)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "        \n",
    "        # 80/20 split\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        print(f\"Dataset created: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def create_univariate_model(self, params, input_shape, output_shape):\n",
    "        \"\"\"Create SimpleRNN model for univariate prediction\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First SimpleRNN layer\n",
    "        if params.get('units_2', 0) > 0:\n",
    "            # Two-layer architecture\n",
    "            model.add(SimpleRNN(\n",
    "                params['units_1'], \n",
    "                return_sequences=True, \n",
    "                input_shape=input_shape,\n",
    "                dropout=params.get('dropout', 0.0),\n",
    "                recurrent_dropout=params.get('recurrent_dropout', 0.0)\n",
    "            ))\n",
    "            \n",
    "            if params.get('dropout_layer', 0.0) > 0:\n",
    "                model.add(Dropout(params['dropout_layer']))\n",
    "            \n",
    "            # Second SimpleRNN layer  \n",
    "            model.add(SimpleRNN(\n",
    "                params['units_2'], \n",
    "                return_sequences=False,\n",
    "                dropout=params.get('dropout', 0.0),\n",
    "                recurrent_dropout=params.get('recurrent_dropout', 0.0)\n",
    "            ))\n",
    "        else:\n",
    "            # Single-layer architecture\n",
    "            model.add(SimpleRNN(\n",
    "                params['units_1'], \n",
    "                return_sequences=False, \n",
    "                input_shape=input_shape,\n",
    "                dropout=params.get('dropout', 0.0),\n",
    "                recurrent_dropout=params.get('recurrent_dropout', 0.0)\n",
    "            ))\n",
    "        \n",
    "        # Optional dense layers (can now use larger sizes!)\n",
    "        if params.get('dense_units_1', 0) > 0:\n",
    "            model.add(Dense(params['dense_units_1'], activation='relu'))\n",
    "            if params.get('dropout_dense_1', 0.0) > 0:\n",
    "                model.add(Dropout(params['dropout_dense_1']))\n",
    "        \n",
    "        if params.get('dense_units_2', 0) > 0:\n",
    "            model.add(Dense(params['dense_units_2'], activation='relu'))\n",
    "            if params.get('dropout_dense_2', 0.0) > 0:\n",
    "                model.add(Dropout(params['dropout_dense_2']))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(output_shape))\n",
    "        \n",
    "        # Compile\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    def estimate_vram_usage(self, params):\n",
    "        \"\"\"Estimate VRAM usage for univariate model\"\"\"\n",
    "        n_features = 1  # Only q64\n",
    "        past_window = params['past_window']\n",
    "        future_window = params['future_window']\n",
    "        batch_size = params['batch_size']\n",
    "        units_1 = params['units_1']\n",
    "        units_2 = params.get('units_2', 0)\n",
    "        dense_1 = params.get('dense_units_1', 0)\n",
    "        dense_2 = params.get('dense_units_2', 0)\n",
    "        \n",
    "        # Calculate memory in MB\n",
    "        input_mem = batch_size * past_window * n_features * 4 / (1024**2)\n",
    "        \n",
    "        # RNN layers\n",
    "        rnn1_params = (n_features + units_1 + 1) * units_1 * 3 * 4 / (1024**2)\n",
    "        rnn1_states = batch_size * units_1 * 4 / (1024**2)\n",
    "        \n",
    "        rnn2_params = rnn2_states = 0\n",
    "        if units_2 > 0:\n",
    "            rnn2_params = (units_1 + units_2 + 1) * units_2 * 3 * 4 / (1024**2)\n",
    "            rnn2_states = batch_size * units_2 * 4 / (1024**2)\n",
    "        \n",
    "        # Dense layers\n",
    "        dense_params = dense_states = 0\n",
    "        if dense_1 > 0:\n",
    "            last_size = units_2 if units_2 > 0 else units_1\n",
    "            dense_params += (last_size + 1) * dense_1 * 4 / (1024**2)\n",
    "            dense_states += batch_size * dense_1 * 4 / (1024**2)\n",
    "            \n",
    "            if dense_2 > 0:\n",
    "                dense_params += (dense_1 + 1) * dense_2 * 4 / (1024**2)\n",
    "                dense_states += batch_size * dense_2 * 4 / (1024**2)\n",
    "                last_size = dense_2\n",
    "            else:\n",
    "                last_size = dense_1\n",
    "        else:\n",
    "            last_size = units_2 if units_2 > 0 else units_1\n",
    "        \n",
    "        # Output layer\n",
    "        output_params = (last_size + 1) * future_window * 4 / (1024**2)\n",
    "        output_mem = batch_size * future_window * 4 / (1024**2)\n",
    "        \n",
    "        # Total with optimizer overhead\n",
    "        base_memory = (input_mem + rnn1_params + rnn1_states + rnn2_params + \n",
    "                      rnn2_states + dense_params + dense_states + output_params + output_mem)\n",
    "        total_vram = base_memory * 3.5  # Adam optimizer overhead\n",
    "        \n",
    "        return total_vram\n",
    "    \n",
    "    def evaluate_config(self, params):\n",
    "        \"\"\"Evaluate single configuration\"\"\"\n",
    "        try:\n",
    "            # Estimate VRAM usage\n",
    "            vram_mb = self.estimate_vram_usage(params)\n",
    "            if vram_mb > 14000:  # 14GB safety limit\n",
    "                return {\n",
    "                    **params, 'mae': float('inf'), 'rmse': float('inf'), 'r2': -float('inf'),\n",
    "                    'vram_mb': vram_mb, 'status': f'vram_exceeded_{vram_mb:.0f}MB'\n",
    "                }\n",
    "            \n",
    "            arch_str = f\"{params['units_1']}\"\n",
    "            if params.get('units_2', 0) > 0:\n",
    "                arch_str += f\"-{params['units_2']}\"\n",
    "            if params.get('dense_units_1', 0) > 0:\n",
    "                arch_str += f\"-{params['dense_units_1']}\"\n",
    "            if params.get('dense_units_2', 0) > 0:\n",
    "                arch_str += f\"-{params['dense_units_2']}\"\n",
    "                \n",
    "            print(f\"Testing: W={params['past_window']}→{params['future_window']}, \"\n",
    "                  f\"Arch={arch_str}, B={params['batch_size']}, \"\n",
    "                  f\"VRAM~{vram_mb:.0f}MB\")\n",
    "            \n",
    "            clear_gpu_memory()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Create dataset\n",
    "            X_train, X_test, y_train, y_test = self.create_univariate_dataset(\n",
    "                params['past_window'], params['future_window']\n",
    "            )\n",
    "            \n",
    "            # Create model\n",
    "            model = self.create_univariate_model(\n",
    "                params, \n",
    "                (params['past_window'], 1),  # 1 feature (q64 only)\n",
    "                params['future_window']\n",
    "            )\n",
    "            \n",
    "            # Callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss', patience=params.get('patience', 15),\n",
    "                    restore_best_weights=True, verbose=0\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor='val_loss', factor=0.7, patience=5,\n",
    "                    min_lr=1e-6, verbose=0\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=params['epochs'],\n",
    "                batch_size=params['batch_size'],\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test, verbose=0)\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            y_pred_inv = self.scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "            y_test_inv = self.scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            \n",
    "            # Per-timestep performance\n",
    "            timestep_metrics = []\n",
    "            for t in range(min(5, params['future_window'])):\n",
    "                y_test_t = self.scaler.inverse_transform(y_test[:, t].reshape(-1, 1)).flatten()\n",
    "                y_pred_t = self.scaler.inverse_transform(y_pred[:, t].reshape(-1, 1)).flatten()\n",
    "                mae_t = mean_absolute_error(y_test_t, y_pred_t)\n",
    "                r2_t = r2_score(y_test_t, y_pred_t)\n",
    "                timestep_metrics.append({'timestep': t+1, 'mae': mae_t, 'r2': r2_t})\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            epochs_trained = len(history.history['loss'])\n",
    "            \n",
    "            result = {\n",
    "                **params,\n",
    "                'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "                'epochs_trained': epochs_trained, 'training_time': training_time,\n",
    "                'vram_mb': vram_mb,\n",
    "                'final_train_loss': history.history['loss'][-1],\n",
    "                'final_val_loss': history.history['val_loss'][-1],\n",
    "                'timestep_performance': timestep_metrics,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            print(f\"  Results: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, \"\n",
    "                  f\"Time={training_time:.1f}s, Epochs={epochs_trained}\")\n",
    "            \n",
    "            del model\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {str(e)}\")\n",
    "            clear_gpu_memory()\n",
    "            result = {\n",
    "                **params, 'mae': float('inf'), 'rmse': float('inf'), 'r2': -float('inf'),\n",
    "                'status': f'error: {str(e)}'\n",
    "            }\n",
    "        \n",
    "        print()\n",
    "        return result\n",
    "    def run_grid_search(self, param_grid, max_evals=None):\n",
    "        \"\"\"Run grid search for univariate prediction\"\"\"\n",
    "        print(\"Starting Univariate q64 Grid Search\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        grid = list(ParameterGrid(param_grid))\n",
    "        \n",
    "        if max_evals and len(grid) > max_evals:\n",
    "            print(f\"Limiting to {max_evals} evaluations out of {len(grid)} combinations\")\n",
    "            # Sort by VRAM usage (smallest first)\n",
    "            grid.sort(key=lambda p: self.estimate_vram_usage(p))\n",
    "            grid = grid[:max_evals]\n",
    "        \n",
    "        print(f\"Total configurations: {len(grid)}\")\n",
    "        print(f\"Using only: {self.target_feature}\")\n",
    "        print()\n",
    "        \n",
    "        for i, params in enumerate(grid, 1):\n",
    "            print(f\"Configuration {i}/{len(grid)}:\")\n",
    "            result = self.evaluate_config(params)\n",
    "            self.results.append(result)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print results summary\"\"\"\n",
    "        successful = [r for r in self.results if r['status'] == 'success']\n",
    "        failed = [r for r in self.results if r['status'] != 'success']\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"UNIVARIATE q64 GRID SEARCH RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total: {len(self.results)}, Success: {len(successful)}, Failed: {len(failed)}\")\n",
    "        \n",
    "        if successful:\n",
    "            best_r2 = max(r['r2'] for r in successful)\n",
    "            best_mae = min(r['mae'] for r in successful)\n",
    "            avg_vram = np.mean([r.get('vram_mb', 0) for r in successful])\n",
    "            \n",
    "            print(f\"\\nBest R²: {best_r2:.4f}\")\n",
    "            print(f\"Best MAE: {best_mae:.4f}\")\n",
    "            print(f\"Avg VRAM: {avg_vram:.0f}MB\")\n",
    "            \n",
    "            print(f\"\\nTOP 5 MODELS:\")\n",
    "            print(\"-\" * 60)\n",
    "            top_models = sorted(successful, key=lambda x: x['r2'], reverse=True)[:5]\n",
    "            \n",
    "            for i, model in enumerate(top_models, 1):\n",
    "                arch = f\"{model['units_1']}\"\n",
    "                if model.get('units_2', 0) > 0:\n",
    "                    arch += f\"-{model['units_2']}\"\n",
    "                if model.get('dense_units_1', 0) > 0:\n",
    "                    arch += f\"-{model['dense_units_1']}\"\n",
    "                if model.get('dense_units_2', 0) > 0:\n",
    "                    arch += f\"-{model['dense_units_2']}\"\n",
    "                \n",
    "                print(f\"{i}. R²={model['r2']:.4f} | MAE={model['mae']:.4f}\")\n",
    "                print(f\"   W={model['past_window']}→{model['future_window']} | {arch} | \"\n",
    "                      f\"VRAM={model.get('vram_mb', 0):.0f}MB\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_univariate_param_grid():\n",
    "    \"\"\"Parameter grid optimized for univariate q64 prediction\"\"\"\n",
    "    return {\n",
    "        # Your specific window sizes\n",
    "        'past_window': [24, 60, 180],\n",
    "        'future_window': [24, 60, 180],\n",
    "        \n",
    "        # Now you can use MUCH larger architectures!\n",
    "        'units_1': [32, 64],            # Larger first layer\n",
    "        'units_2': [0, 32, 64],              # Larger second layer\n",
    "        \n",
    "        # Multiple dense layers possible now\n",
    "        'dense_units_1': [0, 32, 64],       # First dense layer\n",
    "        'dense_units_2': [0, 32, 64],        # Second dense layer\n",
    "        \n",
    "        # Regularization\n",
    "        'dropout': [0.0, 0.1, 0.2],\n",
    "        'recurrent_dropout': [0.0, 0.1],\n",
    "        'dropout_layer': [0.0, 0.1],\n",
    "        'dropout_dense_1': [0.0, 0.1],\n",
    "        'dropout_dense_2': [0.0, 0.1],\n",
    "        \n",
    "        # Training parameters\n",
    "        'learning_rate': [0.001, 0.005],\n",
    "        'batch_size': [32, 64],               # Can use larger batches now\n",
    "        'epochs': [20, 30],\n",
    "        'patience': [5, 10]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Univariate q64 Time Series Prediction - RTX 5060 Ti Optimized\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize\n",
    "    grid_search = Q64_GridSearch('../data/mucnuoc_gio_preprocess.csv')\n",
    "    \n",
    "    # Parameter grid\n",
    "    param_grid = get_univariate_param_grid()\n",
    "    \n",
    "    print(\"Parameter ranges:\")\n",
    "    for param, values in param_grid.items():\n",
    "        print(f\"  {param}: {values}\")\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for values in param_grid.values():\n",
    "        total_combinations *= len(values)\n",
    "    print(f\"\\nTotal combinations: {total_combinations:,}\")\n",
    "    \n",
    "    # Run grid search\n",
    "    max_evaluations = 75  # Adjust based on time budget\n",
    "    results = grid_search.run_grid_search(param_grid, max_evals=max_evaluations)\n",
    "    \n",
    "    # Print results\n",
    "    grid_search.print_summary()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
